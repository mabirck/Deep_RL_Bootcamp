{
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "name": "",
  "signature": "sha256:1688f946ac0078882ac39e2d2b62e6e98e50092bc6ca04d87d490225a72db23e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "nbgrader": {
       "grade": false,
       "grade_id": "intro_1",
       "locked": false,
       "solution": false
      }
     },
     "source": [
      "# Lab 1: Markov Decision Processes - Problem 2\n",
      "\n",
      "\n",
      "## Lab Instructions\n",
      "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
      "\n",
      "**You should execute every block of code to not miss any dependency.**\n",
      "\n",
      "\n",
      "*This project was developed by Peter Chen, Rocky Duan, Pieter Abbeel for the Berkeley Deep RL Bootcamp, August 2017. Bootcamp website with slides and lecture videos: https://sites.google.com/view/deep-rl-bootcamp/. It is adapted from Berkeley Deep RL Class [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb) [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*\n",
      "\n",
      "--------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# basic setup\n",
      "from misc import FrozenLakeEnv, make_grader\n",
      "env = FrozenLakeEnv()\n",
      "import numpy as np, numpy.random as nr, gym\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "np.set_printoptions(precision=3)\n",
      "class MDP(object):\n",
      "    def __init__(self, P, nS, nA, desc=None):\n",
      "        self.P = P # state transition and reward probabilities, explained below\n",
      "        self.nS = nS # number of states\n",
      "        self.nA = nA # number of actions\n",
      "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
      "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
      "GAMMA = 0.95"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 2: Policy Iteration\n",
      "\n",
      "The next task is to implement exact policy iteration (PI), which has the following pseudocode:\n",
      "\n",
      "---\n",
      "Initialize $\\pi_0$\n",
      "\n",
      "For $n=0, 1, 2, \\dots$\n",
      "- Compute the state-value function $V^{\\pi_{n}}$\n",
      "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
      "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
      "---\n",
      "\n",
      "Below, you'll implement the first and second steps of the loop.\n",
      "\n",
      "### Problem 2a: state value function\n",
      "\n",
      "You'll write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
      "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
      "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
      "\n",
      "You can solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def compute_vpi(pi, mdp, gamma):\n",
      "    A=np.zeros((mdp.nS, mdp.nS))\n",
      "    C=np.zeros((mdp.nS,1))\n",
      "\n",
      "    for state in range(mdp.nS):\n",
      "        # work out quality for each action so we can take the argm\n",
      "        action = pi[state]\n",
      "        # sum over all possible next states (s'), corresponds to the summation over s' in the above equations\n",
      "        A[state,state]=1\n",
      "        for probability, nextstate, reward in mdp.P[state][action]:\n",
      "            A[state][nextstate] += -gamma*probability\n",
      "            C[state] += probability*reward\n",
      "    v= np.linalg.solve(A,C)\n",
      "    return np.reshape(v,mdp.nS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's compute the value of an arbitrarily-chosen policy. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expected_val = np.array([  1.381e-18,   1.844e-04,   1.941e-03,   1.272e-03,   2.108e-18,\n",
      "         0.000e+00,   8.319e-03,   1.727e-16,   3.944e-18,   2.768e-01,\n",
      "         8.562e-02,  -7.242e-16,   7.857e-18,   3.535e-01,   8.930e-01,\n",
      "         0.000e+00])\n",
      "\n",
      "actual_val = compute_vpi(np.arange(16) % mdp.nA, mdp, gamma=GAMMA)\n",
      "if np.all(np.isclose(actual_val, expected_val, atol=1e-4)):\n",
      "    print(\"Test passed\")\n",
      "else:\n",
      "    print(\"Expected: \", expected_val)\n",
      "    print(\"Actual: \", actual_val)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test passed\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2b: state-action value function\n",
      "\n",
      "Next, you'll write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
      "\n",
      "$$Q^{\\pi}(s, a) = \\sum_{s'} P(s,a,s')[ R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_qpi(vpi, mdp, gamma):\n",
      "    Qpi = np.zeros([mdp.nS, mdp.nA]) # REPLACE THIS LINE WITH YOUR CODE\n",
      "    for s in range(mdp.nS):\n",
      "        for a in range(mdp.nA):\n",
      "            Qpi[s][a] = sum([p * (r + gamma * vpi[ns]) for p, ns, r in mdp.P[s][a]])\n",
      "            \n",
      "    return Qpi\n",
      "\n",
      "expected_Qpi = np.array([[  0.38 ,   3.135,   1.14 ,   0.095],\n",
      "       [  0.57 ,   3.99 ,   2.09 ,   0.95 ],\n",
      "       [  1.52 ,   4.94 ,   3.04 ,   1.9  ],\n",
      "       [  2.47 ,   5.795,   3.23 ,   2.755],\n",
      "       [  3.8  ,   6.935,   4.56 ,   0.855],\n",
      "       [  4.75 ,   4.75 ,   4.75 ,   4.75 ],\n",
      "       [  4.94 ,   8.74 ,   6.46 ,   2.66 ],\n",
      "       [  6.65 ,   6.65 ,   6.65 ,   6.65 ],\n",
      "       [  7.6  ,  10.735,   8.36 ,   4.655],\n",
      "       [  7.79 ,  11.59 ,   9.31 ,   5.51 ],\n",
      "       [  8.74 ,  12.54 ,  10.26 ,   6.46 ],\n",
      "       [ 10.45 ,  10.45 ,  10.45 ,  10.45 ],\n",
      "       [ 11.4  ,  11.4  ,  11.4  ,  11.4  ],\n",
      "       [ 11.21 ,  12.35 ,  12.73 ,   9.31 ],\n",
      "       [ 12.16 ,  13.4  ,  14.48 ,  10.36 ],\n",
      "       [ 14.25 ,  14.25 ,  14.25 ,  14.25 ]])\n",
      "\n",
      "Qpi = compute_qpi(np.arange(mdp.nS), mdp, gamma=0.95)\n",
      "if np.all(np.isclose(expected_Qpi, Qpi, atol=1e-4)):\n",
      "    print(\"Test passed\")\n",
      "else:\n",
      "    print(\"Expected: \", expected_Qpi)\n",
      "    print(\"Actual: \", Qpi)"
     ],
     "language": "python",
     "metadata": {
      "nbgrader": {
       "grade": false,
       "grade_id": "compute_qpi",
       "locked": false,
       "solution": true
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test passed\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to run policy iteration!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_iteration(mdp, gamma, nIt, grade_print=print):\n",
      "    Vs = []\n",
      "    pis = []\n",
      "    pi_prev = np.zeros(mdp.nS,dtype='int')\n",
      "    pis.append(pi_prev)\n",
      "    grade_print(\"Iteration | # chg actions | V[0]\")\n",
      "    grade_print(\"----------+---------------+---------\")\n",
      "    for it in range(nIt):        \n",
      "        # YOUR CODE HERE\n",
      "        # you need to compute qpi which is the state-action values for current pi\n",
      "        vpi = compute_vpi(pi_prev, )\n",
      "        pi = qpi.argmax(axis=1)\n",
      "        grade_print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
      "        Vs.append(vpi)\n",
      "        pis.append(pi)\n",
      "        pi_prev = pi\n",
      "    return Vs, pis\n",
      "\n",
      "expected_output = \"\"\"Iteration | # chg actions | V[0]\n",
      "----------+---------------+---------\n",
      "   0      |      1        | -0.00000\n",
      "   1      |      9        | 0.00000\n",
      "   2      |      2        | 0.39785\n",
      "   3      |      1        | 0.45546\n",
      "   4      |      0        | 0.53118\n",
      "   5      |      0        | 0.53118\n",
      "   6      |      0        | 0.53118\n",
      "   7      |      0        | 0.53118\n",
      "   8      |      0        | 0.53118\n",
      "   9      |      0        | 0.53118\n",
      "  10      |      0        | 0.53118\n",
      "  11      |      0        | 0.53118\n",
      "  12      |      0        | 0.53118\n",
      "  13      |      0        | 0.53118\n",
      "  14      |      0        | 0.53118\n",
      "  15      |      0        | 0.53118\n",
      "  16      |      0        | 0.53118\n",
      "  17      |      0        | 0.53118\n",
      "  18      |      0        | 0.53118\n",
      "  19      |      0        | 0.53118\"\"\"\n",
      "\n",
      "Vs_PI, pis_PI = policy_iteration(mdp, gamma=0.95, nIt=20, grade_print=make_grader(expected_output))\n",
      "plt.plot(Vs_PI);"
     ],
     "language": "python",
     "metadata": {
      "nbgrader": {
       "grade": false,
       "locked": false,
       "solution": false
      },
      "scrolled": false
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration | # chg actions | V[0]\n",
        "----------+---------------+---------\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'qpi' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-0a49d3caaffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   19      |      0        | 0.53118\"\"\"\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mVs_PI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpis_PI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnIt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_grader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVs_PI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-10-0a49d3caaffb>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(mdp, gamma, nIt, grade_print)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# you need to compute qpi which is the state-action values for current pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mgrade_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%4i      | %6i        | %6.5f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpi_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mVs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'qpi' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}